from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp, date_format, year, month, dayofmonth, hour, minute
import os
from dotenv import load_dotenv

load_dotenv()
spark = SparkSession.builder.appName("CryptoETL").config("spark.jars.packages", "org.postgresql:postgresql:42.6.0").getOrCreate()
spark.sparkContext.setLogLevel("ERROR") # to suppress long warning

jdbc_url = "jdbc:postgresql://localhost:5432/crypto_analytics"
jdbc_props = {
    "user": os.getenv("POSTGRES_USER"),
    "password": os.getenv("POSTGRES_PASSWORD"),
    "driver": "org.postgresql.Driver"
}

df = spark.read.csv('./data/*.csv', header=True, inferSchema=True) # read all the CSVs

# coins table 
coins_df = (
    df.select(
        col("id").alias("coin_id"),
        "symbol",
        "name"
    )
    .dropDuplicates(["coin_id"])
)

coins_df.write.jdbc(
    url=jdbc_url,
    table="coins",
    mode="append", 
    properties=jdbc_props
)

# time_intervals table 
time_df = (
    df.select(
        to_timestamp("collected_at").alias("timestamp")
    )
    .dropDuplicates()
)

time_intervals_df = (
    time_df
    .withColumn("time_key", date_format("timestamp", "yyyyMMddHHmm").cast("long"))
    .withColumn("date", col("timestamp").cast("date"))
    .withColumn("year", year("timestamp"))
    .withColumn("month", month("timestamp"))
    .withColumn("day", dayofmonth("timestamp"))
    .withColumn("hour", hour("timestamp"))
    .withColumn("minute", minute("timestamp"))
    .select(
        "time_key",
        "timestamp",
        "date",
        "hour",
        "minute",
        "day",
        "month",
        "year"
    )
)

time_intervals_df.write.jdbc(
    url=jdbc_url,
    table="time_intervals",
    mode="append", 
    properties=jdbc_props
)

# crypto_prices table
coins_dim = spark.read.jdbc(
    url=jdbc_url,
    table="coins",
    properties=jdbc_props
)

time_intervals_dim = spark.read.jdbc(
    url=jdbc_url,
    table="time_intervals",
    properties=jdbc_props
)

fact_df = (
    df
    .withColumn(
        "timestamp",
        to_timestamp("collected_at")
    )
    .withColumn(
        "time_key",
        date_format(col("timestamp"), "yyyyMMddHHmm").cast("long")
    )
)

fact_with_coins = (
    fact_df
    .join(
        coins_dim,
        fact_df.id == coins_dim.coin_id,
        "inner"
    )
)

fact_with_coins_and_time = (
    fact_with_coins
    .join(
        time_intervals_dim.select("time_key"),
        on="time_key",
        how="inner"
    )
)

crypto_prices_df = fact_with_coins_and_time.select(
    col("coin_key"),
    col("time_key"),
    col("current_price").alias("price_usd"),
    col("market_cap"),
    col("total_volume").alias("volume_24h"),
    col("price_change_percentage_24h").alias("price_change_pct_24h"),
    col("timestamp").alias("collected_at")
)

crypto_prices_df.write.jdbc(
    url=jdbc_url,
    table="crypto_prices",
    mode="append",
    properties=jdbc_props
)